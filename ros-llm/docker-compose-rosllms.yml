version: '3.3'
services:
  container:
    image: custom_ollama
    container_name: ros-llm
    # command: ollama serve && ollama pull llama3.1 && tail -f /dev/null
    command: tail -f /dev/null   
    environment:
      DISPLAY: $DISPLAY
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix:ro
      - ~/.Xauthority:/root/.Xauthority:ro
      - ~/.tmux:/home/user/.tmux:ro
      - ~/.tmux.conf:/home/user/.tmux.conf:ro
      - ~/.config/nvim:/home/user/.config/nvim:ro
      - ~/.config/coc:/home/user/.config/coc:ro
      - ~/.local/share/nvim/:/home/user/.local/share/nvim/:ro
      # - ~/.shh:/home/user/.ssh:ro
      - ~/ROS-LLM:/root/ROS-LLM
      - /dev/snd:/dev/snd:ro
    network_mode: "host"
